{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8831b3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "85072901",
   "metadata": {},
   "outputs": [],
   "source": [
    "array = [1,2,3,4]\n",
    "tuple = (1,2,3,4)\n",
    "dictionay = ['key':1, 'key2':2]\n",
    "\n",
    "# numpy\n",
    "np.array([1,2,3,4]).reshape(2,2)\n",
    "nums = [1,2,3,3]\n",
    "len(nums)\n",
    "set(nums) # {1,2,3} 형태가 되고, 중복 제거\n",
    "np.unique(nums)\n",
    "np.arange(start=5, stop=20, step=5)\n",
    "np.where(nums==2, \"ok\", \"no\")\n",
    "df['is_setosa'] = np.where(df['Species'=='setosa'], 1, 0)\n",
    "df['is_setosa'] = (df['Species']=='setosa')+0 // 동일하게 작동\n",
    "\n",
    "# pandas\n",
    "# Series\n",
    "pd.Series([1,2,3,4,5], index=['a', 'b', 'c', 'd', 'e'])\n",
    "series.sum(), min(), max(), mean(), std(), skew(), kurt(), unique(), idxmax()\n",
    "ser.isin([1,2]).sum() // 1 또는 2가 있는지 -> sum() 사용할 수 있음\n",
    "\n",
    "# DataFrame\n",
    "df = pd.DataFrame(data=np.array([[1,2,3], [4,5,6], [7,8,9]]), columns=['A', 'B', 'C'])\n",
    "df.drop('A', axis=1, inplace=True)\n",
    "df['E'] = pd.Series([5,6,7], index=df.index)\n",
    "df = df.assign(a = A, b=B,,) // column 추가 하기 \n",
    "df_sub = df.iloc[:,[6,0,4,5,7,8,9]] // column 순서 바꾸기\n",
    "\n",
    "df_dum.iloc[0, ] -> Series\n",
    "df_dum.iloc[[0], ] -> DataFrame\n",
    "\n",
    "Series to DataFrame\n",
    "pd.DataFrame({'email':sf.index, 'list':sf.values})\n",
    "Array to DataFrame\n",
    "pd.DataFrame(array, columns=df.columns)\n",
    "\n",
    "# 조합\n",
    "pd.crosstab(df['A'], df['B'], normalize=True) // 컬럼 조합 보기\n",
    "pd.crosstab(df['cut'], df['clarity'], values=df['price'], aggfunc=pd.Series.mean)\n",
    "df.groupby(['cut', 'clarity'])['price'].mean().reset_index() // 두개 동일함. 구조만 다름\n",
    "\n",
    "df['is_setosa'].value_counts(normalize=True)\n",
    "\n",
    "#합치기\n",
    "df = df.append(a) // 같은 column가진 dataframe 끼리 합치기\n",
    "pd.concat([bike_1, bike_2]) // 아래로 합치기 \n",
    "pd.concat([bike_1.reset_index(drop=True), bike_2.reset_index(drop=True)], axis = 1) // 오른쪽으로 합치기\n",
    "pd.merge(left=df_A, right=df_B, left_on='member', right_on='name', how='inner')\n",
    "\n",
    "# datatime\n",
    "bike['datetime'].str.slice(0,4)\n",
    "bike['datetime'].str[0:4]\n",
    "bike['hour'] = pd.to_datetime(bike['datetime']).dt.hour\n",
    "\n",
    "# empty space 확인\n",
    "(df_origin=='').sum()\n",
    "df_origin.apply(lambda x: x=='').sum(axis=0)\n",
    "\n",
    "# 그 외 \n",
    "dia_agg.sort_values(['cut', 'clarity'], ascending=[True, False]) // sort\n",
    "elec_melt = elc.melt(id_vars=[\"YEAR\", \"MONTH\", \"DAY\"])\n",
    "elec_melt.pivot(index = [\"YEAR\",\"MONTH\", \"DAY\"], columns = \"variable\", values= \"value\").reset_index()\n",
    "df = df.rename(columns = {'Sepal.Length':'SL'}) // col name 바꾸기\n",
    "bike['casual'].astype('str')+'대' // type 바꾸기\n",
    "bike_dum = pd.get_dummies(data=bike, columns=['season'], drop_first=True) // dummy\n",
    "pd.get_dummies(df) // object인 애들을 자동으로 ohe 해줌\n",
    "bike.set_index('datetime') // datetime index로 만들기\n",
    "bike.reset_index(drop=True) // drop True 해야 기존 index가 컬럼으로 안들어감\n",
    "df.applymap(lambda x: x.replace({'Yes':1, 'No':0})) // apply: axis에따라, applymap: axis 개념 X\n",
    "col_cate = X.select_dtypes(exclude='number').columns\n",
    "df.drop_duplicates()\n",
    "df.drop(['col1'. 'col2'], axis=1) // col 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5ccaeebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample\n",
    "df.groupby('season').sample(n=2, random_state=123) // 각 season별로 2개씩 추출\n",
    "df.sample(frac=0.05) // 5프로 추출\n",
    "\n",
    "# train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "df_train, df_test = train_test_split(df, train_size=0.7, random_state=123)\n",
    "\n",
    "# 결측치\n",
    "df.isna().sum() // 컬럼별 결측 갯수\n",
    "df_na.fillna(value={'Sepal_Length':999}) // Sepal_Length 컬럼에 na인 애들을 999로 채워줌\n",
    "df.dropna(any or all) // any: 한개라도 na 면 drop, all: 모두 na 면 drop\n",
    "        \n",
    "# 분위수\n",
    "df['A'].quantile(q=0.25 or 0.5 or 0.75)\n",
    "\n",
    "# model info\n",
    "mode.summary() // 모델 정보\n",
    "Adj.R-squared // 결정 계수\n",
    "Intercept: 절편, 독립변수: 기울기\n",
    "\n",
    "# 상관계수\n",
    "df.corr(method='pearson' or 'spearman' or 'kendall')\n",
    "\n",
    "from scipy.stats import pearsonr, spearmanr, kendalltau\n",
    "pearsonr(df['casual'], df['registered'])\n",
    "\n",
    "# Scaler\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "scaler_mm = MinMaxScaler()\n",
    "mm_df = scaler_mm.fit_transform(df['A'])\n",
    "\n",
    "# Score\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, mean_absolute_error, roc_auc_score, f1_score, silhouette_score\n",
    "mean_squared_error(df_test['Outcome'], pred)**0.5 //Regressor 수치형, 회귀\n",
    "accuracy_score(df_test['Outcome'], pred) // Classifier 명목형(0 or 1)\n",
    "roc_auc_score(y_true=df['is_setosa'], y_score=predict_proba[:,1]) // pred 확률값 그대로 넣음\n",
    "accuracy_score, mean_squared_error, mean_absolute_error, roc_auc_score, f1_score// y_pred: 예측값, y_true: 실제값\n",
    "        \n",
    "# Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_pred, y_true) // 재현율: recall\n",
    "f1_scroe(y_test, y_pred, pos_label='Yes')\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "        \n",
    "# 승산비 odds ratio\n",
    "np.exp(model.params)\n",
    "np.exp(model.coef_)\n",
    "\n",
    "# inverse transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a664be92",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 회귀분석\n",
    "# ols\n",
    "from statsmodels.formula.api import ols\n",
    "model = ols(formula=\"A~B\", data=X_train).fit()\n",
    "pred = model.predict(X_test['B']) // ols의 경우 X_test 전체 넣어도 됨\n",
    "mean_squared_error(y_pred=pred, y_true=X_test['A'])**0.5\n",
    "\n",
    "# dmatrices\n",
    "from patsy import dmatrices\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor as vif\n",
    "y,X = dmatrices(\"A ~ \"+\"+\".join(df.columns[:-1]), data=df, return_type=\"dataframe\")\n",
    "X['Intercept']: 절편\n",
    "df_vif = pd.DataFrame()\n",
    "df_vif['colname'] = X.columns\n",
    "df_vif['VIF'] = [vif(X.values, i) for i in range(X.shape[1])]\n",
    "\n",
    "~ 인경우 예상값:\n",
    "df_test = pd.DataFrame({'carat':[1], 'depth':[60], 'table':[55]})\n",
    "model.predict(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a4b271c",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1307415755.py, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\grape\\AppData\\Local\\Temp\\ipykernel_28984\\1307415755.py\"\u001b[1;36m, line \u001b[1;32m7\u001b[0m\n\u001b[1;33m    silhouette_score(df, model.labels_) // 점수 높을 수록 좋음\u001b[0m\n\u001b[1;37m                                              ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# KMeans\n",
    "from sklearn.cluster import KMeans\n",
    "model = KMeans(n_clusters=3, random_state=123)\n",
    "model.fit(df)\n",
    "df['cluster'] = model.labels_\n",
    "df.groupby('cluster')['A'].mean()\n",
    "silhouette_score(df, model.labels_) // 점수 높을 수록 좋음\n",
    "df_centers = pd.DataFrame(model.cluster_centers_, columns=df.columns) // 중심점 위치를 DataFrame으로 만듦\n",
    "\n",
    "# 로지스틱 회귀\n",
    "statsmodel.Logit(), sklearn.LogisticRegression()\n",
    "# Logit\n",
    "model = Logit(endog=X_train['Outcome'], exog=X_train[['A', 'B', 'C']]).fit()\n",
    "pred = model.predict(exog=X_test[['A', 'B', 'C']])\n",
    "pred_class = (pred>0.5)+0\n",
    "accuracy_score(y_true=X_test['Outcome'], y_pred=pred_class)\n",
    "# LogisticRegression\n",
    "model = LogisticRegression().fit(X=X_train[['A','B','C']], y=X_train['Outcome']) // X: 독립변수, y: 종속변수\n",
    "model.coef_: 기울기, model.intercept_: 절편\n",
    "pred = model.predict_proba(X_test[['A','B','C']]) // predict_proba 두번쨰 열이 종속변수가 1이될 확률값, 0.5 기준이면 predict_proba>0.5 == predict\n",
    "pred_class = (pred[:,1]>0.5)+0\n",
    "accuracy_score(y_true=X_test['Outcome'], y_pred=pred_class)\n",
    "\n",
    "# GaussianNB\n",
    "LR와 동일\n",
    "\n",
    "# KNeighbors\n",
    "KNeighborsRegressor(), KNeighborsClassifier() // 수치형, 명목형. Gaussian, LR와 동일, n_cluster = k\n",
    "// test 들어오기 전까지 대기. 학습 안함\n",
    "// 맨하튼 p=1, 유클리드 p=2\n",
    "\n",
    "# DecisionTree\n",
    "DecisionTreeRegressor(), DecisionTreeClassifier() // KNeighbors와 동일하게 사용, max_depth=depth 설정\n",
    "\n",
    "# Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "model = RandomForestClassifier(min_samples_leaf=10, n_estimators=10)\n",
    "pd.Series(index=model.feature_names_in_, data=model.feature_importances_).idxmax() // 변수 중요도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4a22bb7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df3' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_28984\\720708190.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mX\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdf3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'sales'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0mdf3\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'sales'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.7\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m123\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mcol_cate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect_dtypes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minclude\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'object'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df3' is not defined"
     ]
    }
   ],
   "source": [
    "X=df3.drop(columns='sales').copy()\n",
    "y =df3['sales'].copy()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, random_state=123)\n",
    "\n",
    "col_cate = X_train.select_dtypes(include='object').columns\n",
    "col_name = X_train.select_dtypes(exclude='object').columns\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ohe = OneHotEncoder(handle_unknown='ignore', sparse=True)\n",
    "X_train_cate = ohe.fit_transform(X_train[col_cate])\n",
    "X_test_cate = ohe.fit_transform(X_test[col_cate])\n",
    "\n",
    "mm = MinMaxScaler()\n",
    "X_train_num = mm.fit_transform(X_train[col_name])\n",
    "X_test_num = mm.fit_transform(X_test[col_name])\n",
    "\n",
    "from scipy import sparse\n",
    "X_train=sparse.hstack([X_train_cate, X_train_num])\n",
    "X_test=sparse.hstack([X_test_cate, X_test_num])\n",
    "\n",
    "scores = []\n",
    "for k in k_values:\n",
    "    model_knnr = KNeighborsRegressor(n_neighbors=k)\n",
    "    model_knnr.fit(X=X_train, y=y_train)\n",
    "    pred = model_knnr.predict(X_test)\n",
    "    scores[k] = mean_squared_error(y_test, y_pred)**0.5\n",
    "\n",
    "sorted(scores.item(), key=lambda x:x[1])[0]\n",
    "pd.DataFrame(index=scores.keys(), data=scores.values(), columns=['MSE']).sort_values(by='MSE').idxmax()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbd8a17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
